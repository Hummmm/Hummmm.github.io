
---
title: 头戴式MR
tags: AR VR MR
edit: 2020-03-01
categories: 技术类
status: Paused
mathjax: true
highlight: true
mermaid: true
description: 主要是写一些MR方面的思考，另外将头戴式MR和手机端MR比较
---
选特定场景逻辑:在自动驾驶没有做到完美的Inside-Out定位方案之前，移动端的SLAM是不可能适配任何场景的。
# 深度传感器的必要性？
## 主流方案
首先我们来讲一下移动端的深度传感，一般有结构光、TOF、双目3种主流方案：
- 结构光(Structured Light)：结构光投射特定的光信息到物体表面后，由摄像头采集。根据物体造成的光信号的变化来计算物体的位置和深度等信息，进而复原整个三维空间。
- TOF(Time Of Flight，飞行时间)：通过专有传感器，捕捉近红外光从发射到接收的飞行时间，判断物体距离。
- 双目测距(Stereo System)：利用双摄像头拍摄物体，再通过三角形原理计算物体距离。
可以说，既有的三种方案各有所长，TOF的响应速度快、精度高，不易受环境光线干扰，但是功耗和成本都比较大；结构光的工业化应用较多；双目立体成像更适合室外强光条件和高分辨率应用，目前主要应用在机器人视觉、自动驾驶等方面。

## 不同角度：
- 算法人员：产品要做一个地面检测的功能，单靠视觉方案？不可能的，地面都没什么特征信息，很容易误判，我们需要深度传感器。
- 硬件人员：加个深度传感多耗电又发热啊，处理器压力也大，配套设备要上来。
- 产品经理：加上深度传感器可以扩展很多功能，但是成本怎么权衡，只能出旗舰版了。
- 市场需求：手机版都加了深度感应看来技术可行，那头戴式加上也可以啊。
个人思考，头戴式的移动端是要贴皮肤的，切忌牺牲用户舒适度来换取功能提升，不管是从机子重量，温度和耗电量角度，都是对硬件的高要求。
此外，头戴式的移动端在哪些场景需要深度感应器？主要是地面检测辅助定位，3D手势识别辅助真的是市场最急需的吗？
- 从教育角度，如果一个学生看书可以在空中浮现虚拟影像辅助他们，练习英语口语可以看到一个虚拟人和你通过麦克风练习是不是可以了？
- 从游戏的角度，我们非要在没把室内固定场景做稳定的情况下把场景移植到室外不可控环境吗？如果在室内，桌面游戏，贴墙感难道不能通过事先让消费者带着设备转几圈扫描一下获取整个场景深度信息吗？室内场景可控性高一些，这样深度传感器的优势就不明显了。
- 从商业角度，远程会议的主要功能是增加会议人员之间的互动感，这个需要靠沉浸感，模型真实度（暂时不考虑真实3D人体扫描传输，成本过高，摄像头捕捉表情驱动模型更具有实时性）一起完成，人之间的距离是可以自设的，手势和定位不是核心问题。
- 从工业角度，指望一个工人在现场记忆好几个3D手势，而且还必须把手放在眼前才能检测到？那还不如直接给他们一个控制器手柄，拿到的时候就能马上适应。
所以，从商业落地的角度看，除了给开发人员的旗舰版需要深度传感器，大部分真的没必要。市场的产品是需要不断试水的，每一代在前一代的基础上优化增加功能，但前提是每一代产品功能稳定。

## 手机端对比：
头戴式的确是可以加深度传感的，但是我们需要思考落地场景有什么区别。
- 距离：手机端要求用户和手机距离够近，并且现在主要是应用在人脸和手势识别，以及物件扫描。 而对于头戴式设备，手和设备的距离要更远，那么对深度传感器精度要求更高，成本也就上去，设备配置也上去了。
- 贴合度：其中有一个最不可以忽略的技术就是虚拟和现实的贴合度。可以说手机端是没有这个烦恼的，因为本身就是在屏幕上渲染成2D画面，没有eye box，光波导，亮度高等技术壁垒。可是转换到头戴式MR就会涉及很多，要好的贴合度就要好的成像透镜甚至是眼球跟踪技术。
- 亮度：高亮度的虚拟物体体验感才好，但需要功耗高的光机同时会带来设备发热的问题，它是贴合人额头的，虽然现在有很多方案在硬件设计层面避规这个问题。在手机端由于手机壳隔离再加只是在机子边缘接触，那么感受并不会那么强烈。目前这块做的比较好的是Hololens2.
- 实时性：同时还有异步时间扭曲等技术来处理虚拟物体的渲染，头戴式不像手机，人的头部动作往往很迅速，这就对渲染实时的要求加大了。
- 兼容性：当安卓系统外接摄像头时，很多接口是需要许可证等处理，这就需要资本的运作参与，但手机端完全没这个考虑。


# 高质量相机的重要性
- Rolling shutter vs Global shutter
https://blog.csdn.net/lwx309025167/article/details/80257549

FOV对手势识别和场景识别的影响
耗电量
定焦，进光量，和各种库的适配性
3.人体移动速度
滤波的重要性几乎为0，移动太快


提前构建环境，完全没问题，不需要回环只要重定位
目前市面上的android手机多种多样，硬件越来越强大，使用人数也是最多，同时也有前人经验将orb-SLAM2移植到手机上的经验，移植过的人因该都知道，使用的时候，加载词袋模型需要花费7-8分钟时间，变成二进制文件也是缓慢，然后出来效果是每秒1-2帧（记不太请），慢的可以，然后果断放弃了。
然后 开始在手机端重写几乎所有算法，框架仿照ORB-SLAM2，以用来更加容易的适用手机的所有的特性，若是想要达到实时效果或者稍有延迟，只有两种路可以选择 1,降低图像的采样率 2,增加手机处理速度，面对需要用在实际中的项目，只有采用谦前者，果断采用每秒10帧采样，并对图像进行压缩，并使用多线程处理，结果效果不好，采样率只有再降～，采样降低势必造成一些精度损失，只能使用其他传感器进行弥补，所以走到了多传感器融合的道路。

除相机以外，时常会添加IMU（惯性测量单元，用于感知设备的加速度和旋转）来辅助定位。将视觉与IMU融合可以减小低动态时IMU的误差累积，IMU则可以减小高动态时视觉的误差。同时IMU的角速度可以解决单目SLAM的二义性，加速度还可以提供重力方向参考，解决单目VO的尺度问题。IMU传感器使用前也需要校准，并要与相机进行时钟同步
https://blog.csdn.net/hly453/article/details/88983123
人多，夜晚？
slam++
