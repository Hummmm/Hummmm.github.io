---
title: 头戴式混合现实上
tags: MR SLAM
edit: 2020-03-01
categories: 技术类
status: Completed
mathjax: true
highlight: true
mermaid: true
description: 主要是写一些关于设计头戴式MR产品方面的思考以及遇到的坑，比如到底要不要使用深度传感器，合适的落地场景，每个传感器选择和使用的注意项，解析SLAM算法在MR中的结构，另外和手机端MR做比较。
---
首先讲一下我写这篇文章的动机是认为感知行业内一部分文章是单独围绕传感器，SLAM技术，硬件配置单个知识点开展的，或者各个行业(比如无人机，无人驾驶)偏概念化的趋势预测；一部分是在移动端的感知技术很多集中在手机端而非头戴式硬件方案。所以我想写一篇从整体产品导向到技术细化的文章来介绍一下移动端混合现实领域的感知部分技术，而头戴式只是移动端的一个部分。

# 移动端混合现实

## 技术瓶颈

移动端混合现实面临的共同问题也是最重要的问题在我看来就是精确定位和计算消耗之间的矛盾，然而一个好的定位又可以从硬件和软件算法上讨论。

## 硬件的选择

- 卷帘相机 vs 全局快门：
关于这两者的区别大家可以自行搜索，如果用一句通俗易懂的话解释，前者得到的画面是一行行记录感光器的，后者是整张图像一次性记录到感光器。很多时候，我们会遇到设备不是平稳慢速运动的情况，如果不做任何措施使用卷帘相机，画面就会变得模糊。举几个例子，无人机可以通过机械防抖消除一部分因为无人机震动导致的相机捕捉画面糊的情况，可是额外增加了硬件设施；手持手机拍摄通过软件方案（利用MEMS和图像学等知识）来把原始图像矫正回来，可是裁剪了画面大小。头戴式设备不像手机，用户头部是毫无规律的快速的，运动相机是不是全局快门就显得至关重要，画面不会模糊，虽然设备成本上去了。
- 相机参数：在任何感知领域，良好的输入端图像是设备定位准确的前提。理想的相机是高帧率，高进光率，高分辨率，无畸变，大视场角。可是现实往往不可能全部达到，高帧率和高进光率本身是矛盾的，无畸变和大视场角也是矛盾的。
- 相机数量：获取空间信息最好是双目及以上，因为可以获得特征的深度信息。单目摄像头更适合看图出一些简单特效，而且这些特效对深度和贴合度要求不高。但是多目摄像头就对计算单元的压力比较大，我们也可以通过FPGA或者GPU加速实现实时计算输出。经验之谈，如果是固定的室内场景（非空旷大场馆，有大量透明玻璃窗这种极端情况），双目其实满足要求了。
- RGBD摄像头：成本较高，目前移动端的深度传感器方案有结构光、TOF两种。它们各有所长，TOF的响应速度快、精度高，不易受环境光线干扰，但是功耗和成本都比较大；结构光的工业化应用较多。它肯定能减小纯双目SLAM的计算量，但RGBD相机普遍在室外和人多的时候表现效果不佳，更多用于室内环境，之后我会详细写一篇关于采用RGBD SLAM 方案性价比的文章。

![caption](https://github.com/Hummmm/Hummmm.github.io/blob/master/_posts/2017-02-11-Bow/assets/result.png?raw=true)
> 数码相机所拍的直升机，桨叶因果冻效应（卷帘相机）而呈现扭曲变形(来自wiki)
## 软件的选择

 inside out定位方案目前普遍是选SLAM技术，一般SLAM算法包括以下步骤：1）传感器信息读取 2）前端视觉里程计 3）后端优化4）回环检测5）建图

假如同时运行以上五个步骤，势必对处理单元造成压力。有前人经验将orb-SLAM2移植到手机上，移植过的人应该都知道，加载词袋模型需要花费时间长，变成二进制文件好一些。但若是想要达到实时效果或者稍有延迟，有三种路可以选择 1,降低图像的采样率 2,增加手机处理速度3.多传感器融合的道路。
- 选择1，于是对图像进行压缩，多线程处理，采样降低势必造成一些精度损失。
- 选择2，随着手机性能的不断提升以及ARVR库的优化，这个选择是手机端值得关注的。
- 选择3，理论上来说IMU提供了冗余的运动信息，通过数据融合可以得到更加精确的运动估计。但在使用的时候一定是分场合使用的，并且开发人员最好对SLAM的技术有大量的经验，才能知道什么时候用怎么用才能避开IMU带来的弊端。

### IMU+vSLAM方案的局限性
早期的SLAM可以说本质就是研究卡尔曼滤波，粒子滤波这些技术，多用于大型慢速机器导航而不是小型移动端。但随着vSLAM的准确度不断提升，纯滤波研究的人就少了，可是vSLAM的计算量大和运动限速这些弊端慢慢也暴露出来，于是就有人思考将滤波和vSLAM结合。

其中IMU和摄像头配合定位算是比较主流的结合方案，方案分为松耦合和紧耦合，当IMU精度足够高的时候任何一种方式都能帮助定位。但个人觉得不管哪种对移动端定位精度提升效果不大，理由如下：
- IMU适合运动平滑一些的情况，滤波可以预测设备下一秒的状态并且平滑轨迹，可是一方面人体移动速度大于大部分的机器人和手机，并且毫无规律，因此滤波的重要性几乎为0。这就好比股市里用长线思维去实行超短线操作，结果可想而知。
- 事实上是平滑运动时候IMU数据反而会增加误差，就拿Vins-SLAM而言，在慢速运动时候加入 IMU 的版本均不如原始双目版本的精度。原因是原始 VO 的优化已经比较彻底，IMU 误差项的加入给优化结果造成了更多的不稳定性。这一点在作者论文中也能看到，加入 IMU 的版本并没有比原始相机精度更高。
- IMU提供重力分量？首先加速度仪在静止的时候的确可以计算出重力加速度的方向，可是有谁带着设备一动不动呢？另外由于运动时候加速度仪提供的数据是物体运动加速度加重力的叠加，而加速度仪噪音极大并且存在延时，要想从中分解出一个重力方向，不太现实。
- 但是在快速运动的时候，相机方案可能完全失灵，IMU可以减少一些丢失，虽然精度上不是特别理想但总比丢失好；

### 定位算法架构方案

移动端的定位算法需要兼顾尽可能低计算量的同时保证定位准确，然而这两者在传统SLAM中就是矛盾的。我们从SLAM最初的动机开始聊起，SLAM一开始是辅助机器人在位置环境里建图并且定位的，可是人类生活在一个完全已知的环境，我们完全可以先把已知的环境数据输入设备让机器记住这个有尺度的地图，这个就相当于提前完成建图和回环检测步骤。

因此针对MR的落地场景，我们其实只需要选择传感器信息读取，前端视觉里程计和后端优化，有时候后端优化也没必要。 我们需要思考移动端设备应用场景再谈后端优化的必要性。假如文旅场景下，大部分情况是是局部位置出现特效然后一闪而过，接下来随着用户行走，出现的新特效和之前毫无关联，那为什么我们要做后端优化呢？

后端优化的步骤只是把不同时刻的轨迹和地图优化，现在地图不需要优化，前后时刻的特效也无关联。当然，如果特效有事件前后的属性，后端优化可以在时间连续性上起到作用。

# 头戴式混合现实

## 落地场景
我们看一下各个领域头戴式混合现实的落地场景是什么样的。
- 从教育角度，如果一个学生看书可以在空中浮现虚拟影像辅助他们；练习英语口语可以看到一个虚拟人和学生通过麦克风练习。
- 从游戏的角度，大部分游戏还是基于室内设计的,室外就需要考虑续航问题。即使室内可能还需要outside in的方案辅助用户在灯光昏暗的时候定位自己。
- 从商业角度，远程会议的主要功能是增加会议人员之间的互动感，这个需要靠沉浸感，模型真实度（暂时不考虑真实3D人体扫描传输，成本过高，摄像头捕捉表情驱动模型更具有实时性）一起配合完成，人之间的距离是可以自设的，手势和定位不是核心问题。
- 从文旅角度，在解决续航问题以后，设备能及时给客户提供地理上的活动信息，游客走到哪里或者想去哪里会有箭头文字提示，看到文物有相关介绍，已经是很好的辅助导游了。

从商业落地的角度看，头戴式混合现实技术的确提供了目前手机端无法实现落地场景可能性。之前看到一个让亲人看到的还原死去故人的模样和声音的视频，我的确是被感动到，虽然这种Demo还不能达到量产落地，但这种头戴式混合现实独有的沉浸感互动感一定会是未来的交互新趋势。

## 更多自身问题

市面上已经有很多手机端的MR方案，比如ARcore，ARkit，但是这些技术平台的成熟并不意味着解决了头戴式混合现实技术瓶颈。先不说兼容性问题，即当安卓系统外接摄像头时，很多接口是需要许可证才能进行图像处理，这是需要资本合作参与的；本身头戴式设备在开发的过程中就会遇到很多手机端不会遇到的问题。

- 距离：目前隔空操作是手机端研发的一个方向，主要是应用在人脸，手势识别以及物件扫描。手机端要求用户和手机距离够近就可以了， 而对于头戴式设备，手和设备的距离要远得多，那么对深度传感器（比如结构光或者TOF）精度要求更高，设备配置成本也就上去了。
- 贴合度：判断混合现实精度的一个指标就是虚拟和现实物体的贴合度。可以说手机端是没有这个烦恼的，因为本身就是在屏幕上渲染成2D画面。可是转换到头戴式MR就不得不挑战光学技术的壁垒，要好的贴合度就要大的FOV,大eye box的光学模组，同时还要兼顾视线的深度感知。由于篇幅问题，关于光学模组我会单独开一篇来写。
- 亮度：高亮度的虚拟物体会增加用户体验感但需要功耗高的发光模组，这就会带来设备发热的问题。在手机端由于手机壳隔离大屏散热，机子发烫的感觉那么强烈。头戴式是直接把两个激光投影仪塞在眼镜里，硬件贴合人额头的所以必须做好温控，虽然现在有很多方案在结构设计层面( 比如以牺牲体积和成品率的大曲面类透镜方案)让用户暂时感受不到这个问题，但设备本身发热的问题没有得到根本性解决。如果不考虑成本，目前在光能利用率上做的比较好的是微软和索尼。
- 实时性：头戴式不像手机，人的头部动作往往很迅速，对视觉变化敏感，这就对实时渲染的帧率要求加大了。异步时间扭曲（ATW）等技术可以处理虚拟物体的渲染，可以说Oculus在ATW技术上一直做在最前端。

其实除了以上问题还有设备佩戴舒适度，近视人群视线聚焦等问题，但暂时不是这篇文章的重点所以先略过讨论。

## 附加功能的必要性

目前市场对于AR期望是过热的，有一点就体现在很多人希望手机端的MR功能都能在头戴式移动端实现，其中B端客户对手势识别，平面识别的需求特别明显，尤其是当Hololens把这两个功能带入开发者平台以后。目前Hololens是B端市场的定位，需要对使用者进行培训，但是面对开发者的这些功能是不是真的可以被市场接受？

为了得出结论我们做过一个简单的试探性实验，我们首先测试了手势识别。把手掌这个手势识别功能加到眼镜端。然后在高交会和CES上让客户体验，当设备告诉客户伸出手掌放在眼镜前的时候。有的客户只是把手抬到胸口位置，由于位置过低，眼镜的前置RGB摄像头并不能捕捉到客户的手。有的客户只是微张开手指但看起来就像拳头一样，设备并不能把这样的手势识别为手掌而判断成其他手势。其他的情况也层出不穷，并不是技术的错，而是市场还没做好准备来习惯这种指令。那么我们设想一下需要紧急远程工业协助时候的一个场景，一个工人在现场记忆好几个3D手势，一个手势是点击，一个手势是界面推出，一个手势是确认，一个....而且还要时刻注意因为某些手势特殊性放下坏掉的零件把手放在摄像头眼前，到最后工人很可能手忙脚乱。但如果这个时候他们拿到类似游戏手柄的控制器，他们很可能因为玩过switch类似的游戏马上就知道怎么上手。

再然后就是基于地面检测的游戏来试探客户反应，相比于一直需要低头看着地面的塔防射击游戏，客户更倾向于空中射击游戏，不管从沉浸感还是互动感，我也认为射击游戏更刺激有趣味一些。从市场角度看，似乎真的没有多少人介意操作界面是贴着墙还是悬在空中。但只有一次是在做鬼屋项目的时候，为了实现虚拟的鬼手从墙面钻出来，墙面识别还是有必要的。总体而言，平面识别技术真的落地效果好的不多。

当然不是说要保持悲观态度，这些附加功能依旧是可以尝试的技术，技术可行性很高，只是需要等待市场C端对这块的需求提高。

# 后续会写的
- 光学模组的介绍和选择[<sup>1</sup>](#refer-anchor-1)
- ATW的原理介绍，另外还会讲一下如何根据眼球跟踪来进行视野外渲染模糊降低内存占用。
- 环境中有多人存在的情况定位方案，灯光昏暗的情况该如何处理详细讲解。
- 对比手势识别和控制器识别，讨论哪个更适合头戴式混合现实。其中手势会涉及昏暗情况图像增强，小目标检测等技术。
- 讨论自动驾驶和混合现实的关系,以及激光SLAM的介绍，UWB等outside in定位方案介绍。
- 未来头戴式混合现实的发展方向，和IOT,云计算技术的关系

## 参考

<div id="refer-anchor-1"></div>

- [1] [百度学术](http://xueshu.baidu.com/)

<div id="refer-anchor-2"></div>

- [2] [Wikipedia](https://en.wikipedia.org/wiki/Main_Page)